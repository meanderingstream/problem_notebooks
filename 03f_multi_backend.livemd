# LinReg from Nx Examples regression.exs Mult-backend

```elixir
Mix.install(
  [
    {:exla, "~> 0.5.1"},
    {:torchx, "~> 0.5.1"}
  ],
  system_env: %{"XLA_TARGET" => "cuda118"},
  system_env: %{"LIBTORCH_TARGET" => "cu116"}
)
```

## From exla/examples/regression.exs

```elixir
# defmodule Longwings.CassiusBlue.Device do
#   defstruct ~w[backend default_device_type opts]a

#   def exla(default_device_type, opts \\[]) do
#     # Need to validate the default_device_type is :gpu or :cpu
#     opts = validate(opts)

#     case default_device_type do
#       :gpu -> %__MODULE__{
#         backend: EXLA.Backend,
#         default_device_type: [client: :cuda],
#         opts: opts
#       }
#       :cpu -> %__MODULE__{
#         backend: EXLA.Backend,
#         default_device_type: [client: :host],
#         opts: opts
#       }
#     end
#   end

#   def torchx(default_device_type, opts \\ []) do
#     # Need to validate the default_device_type is :gpu or :cpu
#     # opts = Keyword.validate!(opts, device_id: 0, lazy_transfers: :never)
#     opts = validate(opts)
#     case default_device_type do
#       :gpu -> %__MODULE__{
#         backend: Torchx.Backend,
#         default_device_type: [device: :cuda],
#         opts: opts
#       }
#       :cpu -> %__MODULE__{
#         backend: Torchx.Backend,
#         default_device_type: [device: :cpu],
#         opts: opts
#       }
#     end
#   end

#   def default(device) do
#     Nx.default_backend({device.backend, device.default_device_type})
#   end

#   def jit(device, function) do
#     parent_module =  device.backend |> Module.split |> Enum.drop(-1) |> Module.concat
#     Nx.Defn.jit(function, compiler: parent_module)
#   end

#   defp validate(opts) do
#     Keyword.validate!(opts, device_id: 0, lazy_transfers: :never)
#   end
# end
```

```elixir
# alias Longwings.CassiusBlue.Device 
```

```elixir
# device = Device.exla(:gpu)
# device = Device.exla(:cpu)
# device = Device.torchx(:gpu)
# Device.default(device)
```

```elixir
# This cell is where we switch between backends
# backend = EXLA.Backend
# default_opts = [client: :cuda] # Default to gpu (only use if available)
# # default_opts = [client: :host] # Default to cpu (use when lazy_transfer chosen)
# jit_opts = [compiler: EXLA]  # Use when model fits in GPU memory
# jit_opts = [compiler: EXLA, lazy_transfer: :always]

# Torchx
backend = Torchx.Backend
# Default to gpu (only use if available)
default_opts = [device: :cuda]
# default_opts = [device: :cpu] # Default to cpu (no advantage if gpu available)
jit_opts = []
```

```elixir
# Nx.default_backend({EXLA.Backend, client: :cuda})
# Nx.default_backend({EXLA.Backend, client: :host})
# Nx.default_backend({Torchx.Backend, device: :cuda})
# Nx.default_backend({Torchx.Backend, device: :cpu})

Nx.default_backend({backend, default_opts})
```

```elixir
Nx.default_backend()
```

```elixir
defmodule LinReg do
  import Nx.Defn

  # y = mx + b
  defn init_random_params do
    key = Nx.Random.key(42)
    {m, new_key} = Nx.Random.normal(key, 0.0, 0.1, shape: {1, 1})
    {b, _new_key} = Nx.Random.normal(new_key, 0.0, 0.1, shape: {1})
    {m, b}
  end

  defn predict({m, b}, inp) do
    Nx.dot(inp, m) + b
  end

  # MSE Loss
  defn loss({m, b}, inp, tar) do
    preds = predict({m, b}, inp)
    Nx.mean(Nx.pow(tar - preds, 2))
  end

  defn update({m, b} = params, inp, tar, step) do
    {grad_m, grad_b} = grad(params, &loss(&1, inp, tar))
    {m - grad_m * step, b - grad_b * step}
  end

  def train(params, epochs, lin_fn, jit_opts) do
    data =
      Stream.repeatedly(fn -> for _ <- 1..32, do: :rand.uniform() * 10 end)
      |> Stream.map(fn x -> Enum.zip(x, Enum.map(x, lin_fn)) end)

    # update_fn = EXLA.jit(&LinReg.update/4)
    # update_fn = Device.jit(device, &LinReg.update/4)
    # update_fn = Nx.Defn.jit(&LinReg.update/4)
    update_fn = Nx.Defn.jit(&LinReg.update/4, jit_opts)
    # update_fn = Nx.Defn.jit(&LinReg.update/4, [])

    for _ <- 1..epochs, reduce: params do
      acc ->
        data
        |> Enum.take(200)
        |> Enum.reduce(
          acc,
          fn batch, cur_params ->
            {inp, tar} = Enum.unzip(batch)
            x = Nx.reshape(Nx.tensor(inp), {32, 1})
            y = Nx.reshape(Nx.tensor(tar), {32, 1})
            # update(cur_params, x, y, 0.001)
            update_fn.(cur_params, x, y, 0.001)
          end
        )
    end
  end
end
```

```elixir
# Nx.default_backend(EXLA.Backend)
# Nx.default_backend({EXLA.Backend, client: :host})
```

```elixir
params = LinReg.init_random_params()
m = :rand.normal(0.0, 10.0)
b = :rand.normal(0.0, 5.0)
IO.puts("Target m: #{m} Target b: #{b}\n")
```

```elixir
lin_fn = fn x -> m * x + b end
epochs = 100
```

```elixir
# These will be very close to the above coefficients
{time, {trained_m, trained_b}} = :timer.tc(LinReg, :train, [params, epochs, lin_fn, jit_opts])
```

```elixir
trained_m =
  trained_m
  |> Nx.squeeze()
  |> Nx.backend_transfer()
  |> Nx.to_number()
```

```elixir
trained_b =
  trained_b
  |> Nx.squeeze()
  |> Nx.backend_transfer()
  |> Nx.to_number()
```

```elixir
IO.puts("Target m: #{m} Target b: #{b}\n")
IO.puts("Trained in #{time / 1_000_000} sec.")
IO.puts("Trained m: #{trained_m} Trained b: #{trained_b}\n")
IO.puts("Accuracy m: #{m - trained_m} Accuracy b: #{b - trained_b}")
```

EXLA GPU  
Trained in 27.674076 sec.  
Used 90% of GPU

EXLA CPU  
Trained in 12.3 sec  
0% of GPU

EXLA GPU w jit  
Trained in 8.435983 sec. 
Used 90% of GPU

EXLA CPU w jit  
trained in 8.780045 sec
Used 90% of GPU
