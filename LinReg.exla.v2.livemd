# LinReg from Nx Examples regression.exs using EXLA

```elixir
Mix.install(
  [
    # {:nx, "~> 0.3.0"},
    # {:axon, "~> 0.2.0"},
    # {:torchx, "~> 0.4.2"}
    {:exla, "~> 0.4.2"}
    # {:exla, "~> 0.3"}
  ],
  system_env: %{"XLA_TARGET" => "cuda118"}
)
```

## Issue Description

Succeeds::

When using this notebook, the processing will allocate memory on the GPU and proceed correctly.  This notebook fixes the shape issue in the loss function.  Notice that the jit function call is not used.

```
LinReg.train(params, epochs_list, lin_fn)
```

The above cell took 73 seconds

```elixir
Nx.default_backend({EXLA.Backend, client: :cuda})
```

```elixir
defmodule LinReg do
  import Nx.Defn

  # y = mx + b
  defn init_random_params do
    m = Nx.random_normal({1, 1}, 0.0, 0.1)
    b = Nx.random_normal({1}, 0.0, 0.1)
    {m, b}
  end

  defn predict({m, b}, inp) do
    inp
    |> Nx.dot(m)
    |> Nx.add(b)
  end

  # MSE Loss
  defn loss({m, b}, inp, tar) do
    preds = predict({m, b}, inp)
    # tar = Nx.reshape(tar, {:auto})
    # preds = Nx.reshape(preds, {:auto})

    # Nx.subtract(tar, preds)
    Nx.reshape(tar, {:auto})
    |> Nx.subtract(Nx.reshape(preds, {:auto}))
    |> Nx.power(2)
    |> Nx.mean()

    # Nx.mean(Nx.power(tar - preds, 2))
  end

  defn update({m, b} = params, inp, tar, step) do
    {grad_m, grad_b} = grad(params, &loss(&1, inp, tar))
    {m - grad_m * step, b - grad_b * step}
  end

  def train(params, epochs, lin_fn) do
    data =
      Stream.repeatedly(fn -> for _ <- 1..32, do: :rand.uniform() * 10 end)
      |> Stream.map(fn x -> Enum.zip(x, Enum.map(x, lin_fn)) end)

    # dbg(epochs)
    for _ <- epochs, reduce: params do
      # for _ <- 1..Nx.to_number(epochs), reduce: params do
      acc ->
        data
        |> Enum.take(200)
        |> Enum.reduce(
          acc,
          fn batch, cur_params ->
            {inp, tar} = Enum.unzip(batch)
            x = Nx.reshape(Nx.tensor(inp), {32, 1})
            y = Nx.reshape(Nx.tensor(tar), {1, 32})
            update(cur_params, x, y, 0.001)
          end
        )
    end
  end
end
```

```elixir
epochs = 100
```

```elixir
params = LinReg.init_random_params()
m = :rand.normal(0.0, 10.0)
b = :rand.normal(0.0, 5.0)
IO.puts("Target m: #{m} Target b: #{b}\n")
lin_fn = fn x -> m * x + b end
```

```elixir
epochs_list = Enum.to_list(1..epochs)
```

```elixir
# These will be very close to the above coefficients
# IO.inspect(EXLA.jit(&LinReg.train/3).(params, epochs_list, lin_fn))
# train_fn = EXLA.jit(&LinReg.train/3)

# train_fn.(params, epochs_list, lin_fn)
LinReg.train(params, epochs_list, lin_fn)
# :ok
# LinReg.train(params, epochs, lin_fn)
```

```elixir
# Stream.repeatedly(fn -> for _ <- 1..32, do: :rand.uniform() * 10 end)
# |> Stream.map(fn x -> Enum.zip(x, Enum.map(x, lin_fn)) end)
# |> Stream.take(1)
# |> Enum.to_list()
```
